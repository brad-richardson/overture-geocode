name: Rebuild Database (DB Swap)

# Creates a new D1 database, loads data optimally, then opens a PR to swap the binding.
# This avoids write amplification from INSERT OR REPLACE on the live database.
#
# Benefits:
# - Zero writes to active production DB during rebuild
# - Eliminates index/FTS maintenance overhead during bulk load
# - PR-based switchover for review and controlled deployment
# - Trivial rollback (just keep the old DB)
# - Ideal for dev iterations when schema/FTS is changing frequently

on:
  workflow_dispatch:
    inputs:
      database:
        description: 'Which database to rebuild'
        type: choice
        required: true
        options:
          - forward
          - reverse
      db_suffix:
        description: 'New database suffix (e.g., v42, 2024-01-15)'
        type: string
        required: true

env:
  CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}

jobs:
  rebuild-forward:
    if: inputs.database == 'forward'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: |
          npm ci
          pip install duckdb pyarrow tomli tomli-w

      - name: Get current database info
        id: current_db
        run: |
          # Use Python to safely parse TOML
          python3 << 'PYEOF'
          import tomllib
          import os

          with open('wrangler.toml', 'rb') as f:
              config = tomllib.load(f)

          for db in config.get('d1_databases', []):
              if db.get('binding') == 'DB_DIVISIONS':
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write(f"old_db_name={db['database_name']}\n")
                      f.write(f"old_db_id={db['database_id']}\n")
                  print(f"Current database: {db['database_name']} ({db['database_id']})")
                  break
          PYEOF

      - name: Create new D1 database
        id: new_db
        run: |
          NEW_DB_NAME="geocoder-divisions-${{ inputs.db_suffix }}"
          echo "Creating new database: $NEW_DB_NAME"

          # Try to create the database
          if npx wrangler d1 create "$NEW_DB_NAME" --json > /tmp/db-create.json 2>&1; then
            NEW_DB_ID=$(jq -r '.uuid' /tmp/db-create.json)
            echo "Created new database with ID: $NEW_DB_ID"
          else
            # Check if it failed because database already exists
            if grep -q "already exists" /tmp/db-create.json; then
              echo "Database $NEW_DB_NAME already exists, fetching info..."
              if ! npx wrangler d1 info "$NEW_DB_NAME" --json > /tmp/db-info.json 2>&1; then
                echo "Error: Failed to get info for existing database"
                cat /tmp/db-info.json
                exit 1
              fi
              NEW_DB_ID=$(jq -r '.uuid' /tmp/db-info.json)
            else
              echo "Error: Failed to create database"
              cat /tmp/db-create.json
              exit 1
            fi
          fi

          # Validate we got a valid UUID
          if [ -z "$NEW_DB_ID" ] || [ "$NEW_DB_ID" = "null" ]; then
            echo "Error: Failed to get valid database ID"
            exit 1
          fi

          echo "new_db_name=$NEW_DB_NAME" >> $GITHUB_OUTPUT
          echo "new_db_id=$NEW_DB_ID" >> $GITHUB_OUTPUT
          echo "Database ready: $NEW_DB_NAME ($NEW_DB_ID)"

      - name: Download divisions data
        run: |
          echo "Downloading global divisions from Overture..."
          duckdb < scripts/download_divisions_global.sql

      - name: Build local index
        run: |
          echo "Building local SQLite index..."
          python scripts/build_divisions_index.py

      - name: Export data (rebuild mode)
        run: |
          echo "Exporting data with plain INSERT statements..."
          python scripts/export_to_sql.py \
            indexes/divisions-global.db \
            exports/divisions \
            --table divisions \
            --mode rebuild

      - name: Validate export files exist
        run: |
          echo "Validating export files..."

          # Check schema files exist
          for schema_file in schema-base.sql schema-indexes.sql schema-fts.sql; do
            if [ ! -f "exports/divisions/$schema_file" ]; then
              echo "Error: Missing required file exports/divisions/$schema_file"
              exit 1
            fi
            echo "  Found: $schema_file"
          done

          # Check at least one data file exists
          DATA_FILES=$(ls exports/divisions/data-*.sql 2>/dev/null | wc -l)
          if [ "$DATA_FILES" -eq 0 ]; then
            echo "Error: No data files found in exports/divisions/"
            exit 1
          fi
          echo "  Found: $DATA_FILES data chunk(s)"

      - name: Apply base schema
        run: |
          echo "Phase 1: Creating tables..."
          npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --file=exports/divisions/schema-base.sql

      - name: Load data chunks
        run: |
          echo "Phase 2: Loading data..."
          CHUNK_COUNT=0
          for chunk in exports/divisions/data-*.sql; do
            if [ -f "$chunk" ]; then
              echo "  Loading $chunk..."
              npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
                --remote \
                --file="$chunk"
              CHUNK_COUNT=$((CHUNK_COUNT + 1))
            fi
          done
          echo "Loaded $CHUNK_COUNT data chunks"

      - name: Create indexes
        run: |
          echo "Phase 3: Creating indexes..."
          npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --file=exports/divisions/schema-indexes.sql

      - name: Build FTS index
        run: |
          echo "Phase 4: Creating FTS and triggers..."
          npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --file=exports/divisions/schema-fts.sql

      - name: Validate new database
        id: validate
        run: |
          echo "Validating new database before deployment..."

          # Check row count
          RESULT=$(npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --command "SELECT COUNT(*) as count FROM divisions" --json)
          ROW_COUNT=$(echo "$RESULT" | jq -r '.[0].results[0].count')
          echo "Row count: $ROW_COUNT"

          if [ "$ROW_COUNT" -lt 1000 ]; then
            echo "Error: Database has fewer than 1000 rows ($ROW_COUNT). Something went wrong."
            exit 1
          fi

          # Check FTS works
          FTS_RESULT=$(npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --command "SELECT COUNT(*) as count FROM divisions_fts WHERE divisions_fts MATCH 'boston'" --json)
          FTS_COUNT=$(echo "$FTS_RESULT" | jq -r '.[0].results[0].count')
          echo "FTS test (boston): $FTS_COUNT matches"

          if [ "$FTS_COUNT" -eq 0 ]; then
            echo "Error: FTS search returned no results. FTS may not be working correctly."
            exit 1
          fi

          # Check tables exist
          TABLES=$(npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --command "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name" --json)
          echo "Tables: $TABLES"

          echo "row_count=$ROW_COUNT" >> $GITHUB_OUTPUT
          echo "Validation passed!"

      - name: Update wrangler.toml
        run: |
          echo "Updating wrangler.toml with new database binding..."
          python scripts/update_wrangler_binding.py \
            DB_DIVISIONS \
            "${{ steps.new_db.outputs.new_db_name }}" \
            "${{ steps.new_db.outputs.new_db_id }}"

          echo ""
          echo "Updated wrangler.toml:"
          cat wrangler.toml

      - name: Create PR for switchover
        id: create_pr
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          BRANCH_NAME="db-swap/forward-${{ inputs.db_suffix }}"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git checkout -b "$BRANCH_NAME"
          git add wrangler.toml
          git commit -m "chore: switch forward geocoding to ${{ steps.new_db.outputs.new_db_name }}"
          git push -u origin "$BRANCH_NAME"

          PR_URL=$(gh pr create \
            --title "Switch forward geocoding to ${{ steps.new_db.outputs.new_db_name }}" \
            --body "$(cat <<EOF
          ## Database Rebuild Summary

          | Metric | Value |
          |--------|-------|
          | New Database | \`${{ steps.new_db.outputs.new_db_name }}\` |
          | Old Database | \`${{ steps.current_db.outputs.old_db_name }}\` |
          | Row Count | ${{ steps.validate.outputs.row_count }} |
          | Database ID | \`${{ steps.new_db.outputs.new_db_id }}\` |

          ## Validation Results
          - ✅ Row count check passed (>1000 rows)
          - ✅ FTS search test passed

          ## Merge Instructions
          1. Review the wrangler.toml change
          2. Merge this PR (worker will auto-deploy with new binding)
          3. Verify the geocoder works as expected
          4. Delete the old database when ready:
             \`\`\`bash
             npx wrangler d1 delete ${{ steps.current_db.outputs.old_db_name }}
             \`\`\`

          ## Rollback
          If issues occur after merge, you can:
          1. Revert this PR (switches back to old DB)
          2. The old database is still available until you delete it
          EOF
          )")

          echo "pr_url=$PR_URL" >> $GITHUB_OUTPUT
          echo ""
          echo "Created PR: $PR_URL"

      - name: Summary
        run: |
          echo ""
          echo "=========================================="
          echo "Rebuild complete!"
          echo "=========================================="
          echo ""
          echo "New database: ${{ steps.new_db.outputs.new_db_name }}"
          echo "Old database: ${{ steps.current_db.outputs.old_db_name }}"
          echo "Row count: ${{ steps.validate.outputs.row_count }}"
          echo ""
          echo "PR created: ${{ steps.create_pr.outputs.pr_url }}"
          echo ""
          echo "Next steps:"
          echo "  1. Review and merge the PR to switch to the new database"
          echo "  2. Verify the geocoder works correctly"
          echo "  3. Delete the old database:"
          echo "     npx wrangler d1 delete ${{ steps.current_db.outputs.old_db_name }}"

  rebuild-reverse:
    if: inputs.database == 'reverse'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: |
          npm ci
          pip install duckdb pyarrow tomli tomli-w

      - name: Get current database info
        id: current_db
        run: |
          python3 << 'PYEOF'
          import tomllib
          import os

          with open('wrangler.toml', 'rb') as f:
              config = tomllib.load(f)

          for db in config.get('d1_databases', []):
              if db.get('binding') == 'DB_DIVISIONS_REVERSE':
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write(f"old_db_name={db['database_name']}\n")
                      f.write(f"old_db_id={db['database_id']}\n")
                  print(f"Current database: {db['database_name']} ({db['database_id']})")
                  break
          PYEOF

      - name: Create new D1 database
        id: new_db
        run: |
          NEW_DB_NAME="geocoder-reverse-${{ inputs.db_suffix }}"
          echo "Creating new database: $NEW_DB_NAME"

          if npx wrangler d1 create "$NEW_DB_NAME" --json > /tmp/db-create.json 2>&1; then
            NEW_DB_ID=$(jq -r '.uuid' /tmp/db-create.json)
            echo "Created new database with ID: $NEW_DB_ID"
          else
            if grep -q "already exists" /tmp/db-create.json; then
              echo "Database $NEW_DB_NAME already exists, fetching info..."
              if ! npx wrangler d1 info "$NEW_DB_NAME" --json > /tmp/db-info.json 2>&1; then
                echo "Error: Failed to get info for existing database"
                cat /tmp/db-info.json
                exit 1
              fi
              NEW_DB_ID=$(jq -r '.uuid' /tmp/db-info.json)
            else
              echo "Error: Failed to create database"
              cat /tmp/db-create.json
              exit 1
            fi
          fi

          if [ -z "$NEW_DB_ID" ] || [ "$NEW_DB_ID" = "null" ]; then
            echo "Error: Failed to get valid database ID"
            exit 1
          fi

          echo "new_db_name=$NEW_DB_NAME" >> $GITHUB_OUTPUT
          echo "new_db_id=$NEW_DB_ID" >> $GITHUB_OUTPUT
          echo "Database ready: $NEW_DB_NAME ($NEW_DB_ID)"

      - name: Download divisions area data
        run: |
          echo "Downloading divisions with area from Overture..."
          duckdb < scripts/download_divisions_area.sql

      - name: Build local reverse index
        run: |
          echo "Building local SQLite index..."
          python scripts/build_divisions_reverse_index.py

      - name: Export data (rebuild mode)
        run: |
          echo "Exporting data with plain INSERT statements..."
          python scripts/export_to_sql.py \
            indexes/divisions-reverse.db \
            exports/divisions-reverse \
            --table divisions_reverse \
            --mode rebuild

      - name: Validate export files exist
        run: |
          echo "Validating export files..."

          for schema_file in schema-base.sql schema-indexes.sql; do
            if [ ! -f "exports/divisions-reverse/$schema_file" ]; then
              echo "Error: Missing required file exports/divisions-reverse/$schema_file"
              exit 1
            fi
            echo "  Found: $schema_file"
          done

          DATA_FILES=$(ls exports/divisions-reverse/data-*.sql 2>/dev/null | wc -l)
          if [ "$DATA_FILES" -eq 0 ]; then
            echo "Error: No data files found in exports/divisions-reverse/"
            exit 1
          fi
          echo "  Found: $DATA_FILES data chunk(s)"

      - name: Apply base schema
        run: |
          echo "Phase 1: Creating tables..."
          npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --file=exports/divisions-reverse/schema-base.sql

      - name: Load data chunks
        run: |
          echo "Phase 2: Loading data..."
          CHUNK_COUNT=0
          for chunk in exports/divisions-reverse/data-*.sql; do
            if [ -f "$chunk" ]; then
              echo "  Loading $chunk..."
              npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
                --remote \
                --file="$chunk"
              CHUNK_COUNT=$((CHUNK_COUNT + 1))
            fi
          done
          echo "Loaded $CHUNK_COUNT data chunks"

      - name: Create indexes
        run: |
          echo "Phase 3: Creating indexes..."
          npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --file=exports/divisions-reverse/schema-indexes.sql

      - name: Validate new database
        id: validate
        run: |
          echo "Validating new database before deployment..."

          RESULT=$(npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --command "SELECT COUNT(*) as count FROM divisions_reverse" --json)
          ROW_COUNT=$(echo "$RESULT" | jq -r '.[0].results[0].count')
          echo "Row count: $ROW_COUNT"

          if [ "$ROW_COUNT" -lt 1000 ]; then
            echo "Error: Database has fewer than 1000 rows ($ROW_COUNT). Something went wrong."
            exit 1
          fi

          # Test a bbox query to verify indexes work
          BBOX_RESULT=$(npx wrangler d1 execute ${{ steps.new_db.outputs.new_db_name }} \
            --remote \
            --command "SELECT COUNT(*) as count FROM divisions_reverse WHERE bbox_xmin <= -71.0 AND bbox_xmax >= -71.0 AND bbox_ymin <= 42.3 AND bbox_ymax >= 42.3" --json)
          BBOX_COUNT=$(echo "$BBOX_RESULT" | jq -r '.[0].results[0].count')
          echo "Bbox query test (Boston area): $BBOX_COUNT matches"

          echo "row_count=$ROW_COUNT" >> $GITHUB_OUTPUT
          echo "Validation passed!"

      - name: Update wrangler.toml
        run: |
          echo "Updating wrangler.toml with new database binding..."
          python scripts/update_wrangler_binding.py \
            DB_DIVISIONS_REVERSE \
            "${{ steps.new_db.outputs.new_db_name }}" \
            "${{ steps.new_db.outputs.new_db_id }}"

          echo ""
          echo "Updated wrangler.toml:"
          cat wrangler.toml

      - name: Create PR for switchover
        id: create_pr
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          BRANCH_NAME="db-swap/reverse-${{ inputs.db_suffix }}"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git checkout -b "$BRANCH_NAME"
          git add wrangler.toml
          git commit -m "chore: switch reverse geocoding to ${{ steps.new_db.outputs.new_db_name }}"
          git push -u origin "$BRANCH_NAME"

          PR_URL=$(gh pr create \
            --title "Switch reverse geocoding to ${{ steps.new_db.outputs.new_db_name }}" \
            --body "$(cat <<EOF
          ## Database Rebuild Summary

          | Metric | Value |
          |--------|-------|
          | New Database | \`${{ steps.new_db.outputs.new_db_name }}\` |
          | Old Database | \`${{ steps.current_db.outputs.old_db_name }}\` |
          | Row Count | ${{ steps.validate.outputs.row_count }} |
          | Database ID | \`${{ steps.new_db.outputs.new_db_id }}\` |

          ## Validation Results
          - ✅ Row count check passed (>1000 rows)
          - ✅ Bbox query test passed

          ## Merge Instructions
          1. Review the wrangler.toml change
          2. Merge this PR (worker will auto-deploy with new binding)
          3. Verify the geocoder works as expected
          4. Delete the old database when ready:
             \`\`\`bash
             npx wrangler d1 delete ${{ steps.current_db.outputs.old_db_name }}
             \`\`\`

          ## Rollback
          If issues occur after merge, you can:
          1. Revert this PR (switches back to old DB)
          2. The old database is still available until you delete it
          EOF
          )")

          echo "pr_url=$PR_URL" >> $GITHUB_OUTPUT
          echo ""
          echo "Created PR: $PR_URL"

      - name: Summary
        run: |
          echo ""
          echo "=========================================="
          echo "Rebuild complete!"
          echo "=========================================="
          echo ""
          echo "New database: ${{ steps.new_db.outputs.new_db_name }}"
          echo "Old database: ${{ steps.current_db.outputs.old_db_name }}"
          echo "Row count: ${{ steps.validate.outputs.row_count }}"
          echo ""
          echo "PR created: ${{ steps.create_pr.outputs.pr_url }}"
          echo ""
          echo "Next steps:"
          echo "  1. Review and merge the PR to switch to the new database"
          echo "  2. Verify the geocoder works correctly"
          echo "  3. Delete the old database:"
          echo "     npx wrangler d1 delete ${{ steps.current_db.outputs.old_db_name }}"
