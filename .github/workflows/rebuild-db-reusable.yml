name: Rebuild Database (Reusable)

on:
  workflow_call:
    inputs:
      database_type:
        description: 'Database type: forward or reverse'
        type: string
        required: true
      db_suffix:
        description: 'New database suffix (e.g., v42, 2024-01-15)'
        type: string
        required: true

env:
  CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}

jobs:
  rebuild:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: |
          npm ci
          pip install duckdb pyarrow tomli tomli-w

      - name: Set database config
        id: config
        run: |
          if [ "${{ inputs.database_type }}" = "forward" ]; then
            echo "binding=DB_DIVISIONS" >> $GITHUB_OUTPUT
            echo "new_db_name=geocoder-divisions-${{ inputs.db_suffix }}" >> $GITHUB_OUTPUT
            echo "table=divisions" >> $GITHUB_OUTPUT
            echo "download_script=scripts/download_divisions_global.sql" >> $GITHUB_OUTPUT
            echo "build_script=scripts/build_divisions_index.py" >> $GITHUB_OUTPUT
            echo "local_db=indexes/divisions-global.db" >> $GITHUB_OUTPUT
            echo "export_dir=exports/divisions" >> $GITHUB_OUTPUT
            echo "has_fts=true" >> $GITHUB_OUTPUT
            echo "branch_prefix=db-swap/forward" >> $GITHUB_OUTPUT
          else
            echo "binding=DB_DIVISIONS_REVERSE" >> $GITHUB_OUTPUT
            echo "new_db_name=geocoder-reverse-${{ inputs.db_suffix }}" >> $GITHUB_OUTPUT
            echo "table=divisions_reverse" >> $GITHUB_OUTPUT
            echo "download_script=scripts/download_divisions_area.sql" >> $GITHUB_OUTPUT
            echo "build_script=scripts/build_divisions_reverse_index.py" >> $GITHUB_OUTPUT
            echo "local_db=indexes/divisions-reverse.db" >> $GITHUB_OUTPUT
            echo "export_dir=exports/divisions-reverse" >> $GITHUB_OUTPUT
            echo "has_fts=false" >> $GITHUB_OUTPUT
            echo "branch_prefix=db-swap/reverse" >> $GITHUB_OUTPUT
          fi

      - name: Get current database info
        id: current_db
        run: |
          python3 << 'PYEOF'
          import tomllib
          import os

          binding = "${{ steps.config.outputs.binding }}"

          with open('wrangler.toml', 'rb') as f:
              config = tomllib.load(f)

          for db in config.get('d1_databases', []):
              if db.get('binding') == binding:
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write(f"old_db_name={db['database_name']}\n")
                      f.write(f"old_db_id={db['database_id']}\n")
                  print(f"Current database: {db['database_name']} ({db['database_id']})")
                  break
          else:
              print(f"Error: Binding {binding} not found")
              exit(1)
          PYEOF

      - name: Create new D1 database
        id: new_db
        run: |
          NEW_DB_NAME="${{ steps.config.outputs.new_db_name }}"
          echo "Creating new database: $NEW_DB_NAME"

          # Try to create the database and capture both stdout and stderr
          HTTP_CODE=$(npx wrangler d1 create "$NEW_DB_NAME" --json 2>/tmp/db-stderr.txt | tee /tmp/db-create.json | jq -r '.uuid // empty' || echo "")

          if [ -n "$HTTP_CODE" ] && [ "$HTTP_CODE" != "null" ]; then
            NEW_DB_ID="$HTTP_CODE"
            echo "Created new database with ID: $NEW_DB_ID"
          else
            # Check stderr for "already exists" error
            if grep -qi "already exists" /tmp/db-stderr.txt /tmp/db-create.json 2>/dev/null; then
              echo "Database $NEW_DB_NAME already exists, fetching info..."
              if ! npx wrangler d1 info "$NEW_DB_NAME" --json > /tmp/db-info.json 2>&1; then
                echo "Error: Failed to get info for existing database"
                cat /tmp/db-info.json
                exit 1
              fi
              NEW_DB_ID=$(jq -r '.uuid' /tmp/db-info.json)
            else
              echo "Error: Failed to create database"
              echo "stdout:" && cat /tmp/db-create.json
              echo "stderr:" && cat /tmp/db-stderr.txt
              exit 1
            fi
          fi

          # Validate we got a valid UUID
          if [ -z "$NEW_DB_ID" ] || [ "$NEW_DB_ID" = "null" ]; then
            echo "Error: Failed to get valid database ID"
            exit 1
          fi

          echo "new_db_id=$NEW_DB_ID" >> $GITHUB_OUTPUT
          echo "Database ready: $NEW_DB_NAME ($NEW_DB_ID)"

      - name: Download data
        run: |
          echo "Downloading data from Overture..."
          duckdb < ${{ steps.config.outputs.download_script }}

      - name: Build local index
        run: |
          echo "Building local SQLite index..."
          python ${{ steps.config.outputs.build_script }}

      - name: Export data (rebuild mode)
        run: |
          echo "Exporting data with plain INSERT statements..."
          python scripts/export_to_sql.py \
            ${{ steps.config.outputs.local_db }} \
            ${{ steps.config.outputs.export_dir }} \
            --table ${{ steps.config.outputs.table }} \
            --mode rebuild

      - name: Validate export files exist
        run: |
          echo "Validating export files..."
          EXPORT_DIR="${{ steps.config.outputs.export_dir }}"

          # Check base schema exists
          if [ ! -f "$EXPORT_DIR/schema-base.sql" ]; then
            echo "Error: Missing $EXPORT_DIR/schema-base.sql"
            exit 1
          fi
          echo "  Found: schema-base.sql"

          # Check indexes schema exists
          if [ ! -f "$EXPORT_DIR/schema-indexes.sql" ]; then
            echo "Error: Missing $EXPORT_DIR/schema-indexes.sql"
            exit 1
          fi
          echo "  Found: schema-indexes.sql"

          # Check FTS schema for forward geocoding
          if [ "${{ steps.config.outputs.has_fts }}" = "true" ]; then
            if [ ! -f "$EXPORT_DIR/schema-fts.sql" ]; then
              echo "Error: Missing $EXPORT_DIR/schema-fts.sql"
              exit 1
            fi
            echo "  Found: schema-fts.sql"
          fi

          # Check at least one data file exists
          DATA_FILES=$(ls $EXPORT_DIR/data-*.sql 2>/dev/null | wc -l)
          if [ "$DATA_FILES" -eq 0 ]; then
            echo "Error: No data files found in $EXPORT_DIR/"
            exit 1
          fi
          echo "  Found: $DATA_FILES data chunk(s)"

      - name: Apply base schema
        run: |
          echo "Phase 1: Creating tables..."
          npx wrangler d1 execute ${{ steps.config.outputs.new_db_name }} \
            --remote \
            --file=${{ steps.config.outputs.export_dir }}/schema-base.sql

      - name: Load data chunks
        run: |
          echo "Phase 2: Loading data..."
          EXPORT_DIR="${{ steps.config.outputs.export_dir }}"
          CHUNK_COUNT=0
          for chunk in $EXPORT_DIR/data-*.sql; do
            if [ -f "$chunk" ]; then
              echo "  Loading $chunk..."
              npx wrangler d1 execute ${{ steps.config.outputs.new_db_name }} \
                --remote \
                --file="$chunk"
              CHUNK_COUNT=$((CHUNK_COUNT + 1))
            fi
          done
          echo "Loaded $CHUNK_COUNT data chunks"

      - name: Create indexes
        run: |
          echo "Phase 3: Creating indexes..."
          npx wrangler d1 execute ${{ steps.config.outputs.new_db_name }} \
            --remote \
            --file=${{ steps.config.outputs.export_dir }}/schema-indexes.sql

      - name: Build FTS index
        if: steps.config.outputs.has_fts == 'true'
        run: |
          echo "Phase 4: Creating FTS and triggers..."
          npx wrangler d1 execute ${{ steps.config.outputs.new_db_name }} \
            --remote \
            --file=${{ steps.config.outputs.export_dir }}/schema-fts.sql

      - name: Validate new database
        id: validate
        run: |
          echo "Validating new database before deployment..."
          DB_NAME="${{ steps.config.outputs.new_db_name }}"
          TABLE="${{ steps.config.outputs.table }}"

          # Check row count
          RESULT=$(npx wrangler d1 execute "$DB_NAME" \
            --remote \
            --command "SELECT COUNT(*) as count FROM $TABLE" --json)
          ROW_COUNT=$(echo "$RESULT" | jq -r '.[0].results[0].count')
          echo "Row count: $ROW_COUNT"

          if [ "$ROW_COUNT" -lt 1000 ]; then
            echo "Error: Database has fewer than 1000 rows ($ROW_COUNT). Something went wrong."
            exit 1
          fi

          # Type-specific validation
          if [ "${{ inputs.database_type }}" = "forward" ]; then
            # Check FTS works
            FTS_RESULT=$(npx wrangler d1 execute "$DB_NAME" \
              --remote \
              --command "SELECT COUNT(*) as count FROM divisions_fts WHERE divisions_fts MATCH 'boston'" --json)
            FTS_COUNT=$(echo "$FTS_RESULT" | jq -r '.[0].results[0].count')
            echo "FTS test (boston): $FTS_COUNT matches"

            if [ "$FTS_COUNT" -eq 0 ]; then
              echo "Error: FTS search returned no results. FTS may not be working correctly."
              exit 1
            fi
            echo "validation_details=FTS search test passed" >> $GITHUB_OUTPUT
          else
            # Test a bbox query to verify indexes work
            BBOX_RESULT=$(npx wrangler d1 execute "$DB_NAME" \
              --remote \
              --command "SELECT COUNT(*) as count FROM divisions_reverse WHERE bbox_xmin <= -71.0 AND bbox_xmax >= -71.0 AND bbox_ymin <= 42.3 AND bbox_ymax >= 42.3" --json)
            BBOX_COUNT=$(echo "$BBOX_RESULT" | jq -r '.[0].results[0].count')
            echo "Bbox query test (Boston area): $BBOX_COUNT matches"
            echo "validation_details=Bbox query test passed" >> $GITHUB_OUTPUT
          fi

          echo "row_count=$ROW_COUNT" >> $GITHUB_OUTPUT
          echo "Validation passed!"

      - name: Update wrangler.toml
        run: |
          echo "Updating wrangler.toml with new database binding..."
          python scripts/update_wrangler_binding.py \
            "${{ steps.config.outputs.binding }}" \
            "${{ steps.config.outputs.new_db_name }}" \
            "${{ steps.new_db.outputs.new_db_id }}"

      - name: Create PR for switchover
        id: create_pr
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          BRANCH_NAME="${{ steps.config.outputs.branch_prefix }}-${{ inputs.db_suffix }}"
          DB_TYPE="${{ inputs.database_type }}"
          NEW_DB="${{ steps.config.outputs.new_db_name }}"
          OLD_DB="${{ steps.current_db.outputs.old_db_name }}"
          ROW_COUNT="${{ steps.validate.outputs.row_count }}"
          NEW_DB_ID="${{ steps.new_db.outputs.new_db_id }}"
          VALIDATION="${{ steps.validate.outputs.validation_details }}"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git checkout -b "$BRANCH_NAME"
          git add wrangler.toml
          git commit -m "chore: switch $DB_TYPE geocoding to $NEW_DB"
          git push -u origin "$BRANCH_NAME"

          # Create PR with proper heredoc formatting (<<- strips leading tabs)
          PR_BODY=$(cat <<-EOF
          ## Database Rebuild Summary

          | Metric | Value |
          |--------|-------|
          | Type | $DB_TYPE |
          | New Database | \`$NEW_DB\` |
          | Old Database | \`$OLD_DB\` |
          | Row Count | $ROW_COUNT |
          | Database ID | \`$NEW_DB_ID\` |

          ## Validation Results
          - ✅ Row count check passed (>1000 rows)
          - ✅ $VALIDATION

          ## Merge Instructions
          1. Review the wrangler.toml change
          2. Merge this PR (worker will auto-deploy with new binding)
          3. Verify the geocoder works as expected
          4. Delete the old database when ready:
             \`\`\`bash
             npx wrangler d1 delete $OLD_DB
             \`\`\`

          ## Rollback
          If issues occur after merge, you can:
          1. Revert this PR (switches back to old DB)
          2. The old database is still available until you delete it
          EOF
          )

          PR_URL=$(gh pr create \
            --title "Switch $DB_TYPE geocoding to $NEW_DB" \
            --body "$PR_BODY")

          echo "pr_url=$PR_URL" >> $GITHUB_OUTPUT
          echo ""
          echo "Created PR: $PR_URL"

      - name: Summary
        run: |
          echo ""
          echo "=========================================="
          echo "Rebuild complete!"
          echo "=========================================="
          echo ""
          echo "Type: ${{ inputs.database_type }}"
          echo "New database: ${{ steps.config.outputs.new_db_name }}"
          echo "Old database: ${{ steps.current_db.outputs.old_db_name }}"
          echo "Row count: ${{ steps.validate.outputs.row_count }}"
          echo ""
          echo "PR created: ${{ steps.create_pr.outputs.pr_url }}"
          echo ""
          echo "Next steps:"
          echo "  1. Review and merge the PR to switch to the new database"
          echo "  2. Verify the geocoder works correctly"
          echo "  3. Delete the old database:"
          echo "     npx wrangler d1 delete ${{ steps.current_db.outputs.old_db_name }}"
